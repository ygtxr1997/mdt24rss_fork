# conf/model/mdtv_da_enc.yaml
_target_: mdt.models.mdtv_da.MDTVDomainAdaptVisualEncoder
_recursive_: false

defaults:
  - language_goal: clip
  - visual_goal: clip
  - img_gen: masked_transformer
  - model: mdtv_transformer
  - domain_adapt: adda


latent_dim: 384
multistep: 10
sampler_type: 'ddim'
num_sampling_steps: 10
sigma_data: 0.5
sigma_min: 0.001
sigma_max: 80
noise_scheduler: 'exponential'
sigma_sample_density_type: 'loglogistic'
use_lr_scheduler: true
act_window_size: 10
cont_alpha: 1.0
masked_beta: 1.0
use_distributed_clip: True
use_text_not_embedding: True
ckpt_path: '/mnt/dongxu-fs1/data-ssd/geyuan/code/mdt24rss_fork/logs/runs/2024-09-08/00-29-39/saved_models/epoch=29_eval_lh/avg_seq_len=1.60.ckpt'  # load pretrained ckpt
#ckpt_path: '/public/home/group_xudong/gavinyuan/code/mdt24rss_fork/logs/runs/2024-09-08/14-48-31/saved_models/epoch=29_eval_lh/avg_seq_len=1.96.ckpt'
seed: ${seed}
perceiver_depth: 6
perceiver_heads: 8
perceiver_dim_head: 64
perceiver_num_time_embeds: 1
perceiver_dim: 384
num_latents: 3

voltron_cache: ${root_home}/pretrained

optimizer:
  _target_: torch.optim.AdamW
  transformer_weight_decay: 0.05
  obs_encoder_weight_decay: 0.05
  learning_rate: 1e-4
  betas: [0.9, 0.9]

lr_scheduler:
  lr_scheduler:
    init_lr: 1e-4  # This is the peak or maximum learning rate
    init_lr_scale: 0.1  # This is the ratio of initial learning rate to peak learning rate
    final_lr_scale: 1e-6  # This is the ratio of final learning rate to peak learning rate
    total_steps: 50000  # Example total steps, adjust as needed
    phase_ratio: "(0.02, 0.08, 0.9)"
    lr: 1e-4