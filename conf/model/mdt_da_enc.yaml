# conf/model/mdt_da_enc.yaml
_target_: mdt.models.mdt_da.MDTDomainAdaptVisualEncoder
_recursive_: false

defaults:
  - language_goal: clip
  - visual_goal: clip
  - img_gen: masked_transformer
  - model: mdt_transformer
  - domain_adapt: wgan  # options:adda,wgan,wgan_1152

latent_dim: 512
multistep: 10
sampler_type: 'ddim'
num_sampling_steps: 10
sigma_data: 0.5
sigma_min: 0.001
sigma_max: 80
noise_scheduler: 'exponential'
sigma_sample_density_type: 'loglogistic'
use_lr_scheduler: true
act_window_size: 10
cont_alpha: 1.0
masked_beta: 1.0
use_distributed_clip: True
use_text_not_embedding: True
#ckpt_path: null
ckpt_path: ${root_home}/code/mdt24rss_fork/logs/runs/2024-10-03/20-59-31/calvin_abc/2024-10-03_20-59-31/checkpoints/epoch=39-step=40000.ckpt
seed: ${seed}

optimizer:
  _target_: torch.optim.AdamW
  transformer_weight_decay: 0.01
  obs_encoder_weight_decay: 0.01
  vis1_lr: 1e-6
  vis2_lr: 1e-5
  act_lr: 5e-7
  betas: [0.9,0.99]

lr_scheduler:
  vis1_lr_scheduler:  # static
    lr_scheduler:
      init_lr: 1e-6  # will be recovered
      init_lr_scale: 0.1  # This is the ratio of initial learning rate to peak learning rate
      final_lr_scale: 1e-3  # This is the ratio of final learning rate to peak learning rate
      total_steps: 20000  # Example total steps, adjust as needed
      phase_ratio: "(0.02, 0.15, 0.9)"
      lr: 1e-6  # This is the peak or maximum learning rate
  vis2_lr_scheduler:  # gripper
    lr_scheduler:
      init_lr: 1e-5  # will be recovered
      init_lr_scale: 0.1  # This is the ratio of initial learning rate to peak learning rate
      final_lr_scale: 1e-3  # This is the ratio of final learning rate to peak learning rate
      total_steps: 20000  # Example total steps, adjust as needed
      phase_ratio: "(0.02, 0.1, 0.9)"
      lr: 1e-5  # This is the peak or maximum learning rate
  act_lr_scheduler:
    lr_scheduler:
      init_lr: 1e-6  # will be recovered
      init_lr_scale: 0.1  # This is the ratio of initial learning rate to peak learning rate
      final_lr_scale: 1e-3  # This is the ratio of final learning rate to peak learning rate
      total_steps: 20000  # Example total steps, adjust as needed
      phase_ratio: "(0.02, 0.1, 0.9)"
      lr: 1e-6    # This is the peak or maximum learning rate